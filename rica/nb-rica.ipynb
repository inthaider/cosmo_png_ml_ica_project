{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RICA: Reconstruction Independent Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Parameter\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reproduces Reconstruction ICA with PyTorch\n",
    "\n",
    "1. Modify `torchvision_path_cifar10` to your cifar10 path, or just any folder (it will download dataset automatically)\n",
    "2. If you do not have a GPU, set `use_gpu=False`. It's going to take more than a few minutes.. If you want to speed\n",
    "   things up a bit:\n",
    "   - change lambdas to just [2.4], this runs the script with a single lambda value only, and gives decent result\n",
    "   - maybe reduce num_epochs to 100\n",
    "   - if you want to run all the lambda values\n",
    "     - reduce patch_size to 8, which is probably 2x faster than 16\n",
    "     - reduce num_epochs to 40\n",
    "\"\"\"\n",
    "\n",
    "use_gpu    = False              # if to use GPU\n",
    "num_epochs = 200                # how long each lambda runs, 200 is probably overkill\n",
    "num_steps  = 20                 # how many lambdas to try\n",
    "patch_size = 16                 # patch size to extract, 16 is max\n",
    "weight_size= patch_size**2      # weight size is number of pixels in a patch (do not change)\n",
    "num_filters = weight_size       # complete-ICA has same number of filters as there are pixels\n",
    "lambdas = [l*0.4 for l in range(1,num_steps)] # the lambda values will be tried one by one\n",
    "torchvision_path_cifar10 = 'torchvision_cifar10/'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to torchvision_cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b174c47373e411cbe6d8aaff9cf7c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting torchvision_cifar10/cifar-10-python.tar.gz to torchvision_cifar10/\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/fs/lustre/cita/haider/projects/pnong_ml/rica/nb-rica.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcita_mussel/fs/lustre/cita/haider/projects/pnong_ml/rica/nb-rica.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# loader = torch.utils.data.DataLoader(dataset, batch_size=1000, num_workers=2, pin_memory=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcita_mussel/fs/lustre/cita/haider/projects/pnong_ml/rica/nb-rica.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcita_mussel/fs/lustre/cita/haider/projects/pnong_ml/rica/nb-rica.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcita_mussel/fs/lustre/cita/haider/projects/pnong_ml/rica/nb-rica.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#     data_all.append(pos)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcita_mussel/fs/lustre/cita/haider/projects/pnong_ml/rica/nb-rica.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# data_all = torch.cat(data_all)      # merge into single tensor\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcita_mussel/fs/lustre/cita/haider/projects/pnong_ml/rica/nb-rica.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m data_all \u001b[39m=\u001b[39m dataset\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcita_mussel/fs/lustre/cita/haider/projects/pnong_ml/rica/nb-rica.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m data_all \u001b[39m=\u001b[39m data_all\u001b[39m.\u001b[39;49mmean(\u001b[39m1\u001b[39m)         \u001b[39m# make black-white\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcita_mussel/fs/lustre/cita/haider/projects/pnong_ml/rica/nb-rica.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m data_all \u001b[39m=\u001b[39m maybe_gpu(data_all)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "def maybe_gpu(data):\n",
    "    return data.cuda() if use_gpu else data\n",
    "\n",
    "\n",
    "# use cifar10 as dataset\n",
    "dataset = torchvision.datasets.CIFAR10(\n",
    "        torchvision_path_cifar10, \n",
    "        train=True, \n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.469, 0.481, 0.451], std=[0.239,0.245,0.272])\n",
    "            # normalize to 0-mean, unit-variance\n",
    "        ]), \n",
    "        download=True)\n",
    "\n",
    "z_chisq_npy = np.load(\"pnong/np-1d_zeta_fields/z_chisq_seeds741785_501982.npy\")\n",
    "z_chisq = torch.Tensor(z_chisq_npy)\n",
    "dataset = TensorDataset(z_chisq) # create your datset\n",
    "loader = DataLoader(dataset) # create your dataloader\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset, batch_size=1000, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "# # load the entire dataset into a single Tensor, this speeds things up quite a bit\n",
    "# data_all = []\n",
    "# for pos in loader:\n",
    "#     data_all.append(pos)\n",
    "# data_all = torch.cat(data_all)      # merge into single tensor\n",
    "data_all = dataset\n",
    "data_all = data_all.mean(1)         # make black-white\n",
    "data_all = maybe_gpu(data_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doit(lambd=1, epochs=num_epochs):\n",
    "    weight    = Parameter(maybe_gpu(1.0/patch_size*torch.Tensor(weight_size,num_filters).normal_()))\n",
    "    optimizer = torch.optim.RMSprop([weight], 0.001, momentum=0.9)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(data_all.size(0)/1000):\n",
    "            # select batch\n",
    "            imgs = data_all[batch*1000:(batch+1)*1000]\n",
    "            # capture a few patches\n",
    "            patches = []\n",
    "            for x,y in itertools.product([0, 8, 16],[0,8,16]):\n",
    "                patches.append(imgs[:, y:y+patch_size, x:x+patch_size])\n",
    "            patches = Variable(maybe_gpu(torch.cat(patches)))\n",
    "            patches = patches.view(patches.size(0), -1)\n",
    "            latents= patches.matmul(weight)\n",
    "            output = latents.matmul(weight.t())\n",
    "            diff = output - patches\n",
    "            loss_recon = (diff * diff).mean()\n",
    "            loss_latent= latents.abs().mean()\n",
    "            loss = lambd * loss_recon + loss_latent\n",
    "            # optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(epoch, loss.data[0], loss_recon.data[0], loss_latent.data[0])\n",
    "    weight_images = weight.data.t().contiguous().view(num_filters, 1, patch_size, patch_size).cpu()\n",
    "    vutils.save_image(weight_images, 'rica_weight_images_{}.jpg'.format(lambd), nrow=patch_size, normalize=True)\n",
    "    print('Finished lambda={}'.format(lambd))\n",
    "\n",
    "\n",
    "for l in lambdas:\n",
    "    doit(l)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 ('cosmic_nong')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6781a5192c0b6eb04b7b9348c292a0e39c03bca97c5ebc8f1c4565fff6f215ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
